{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKDbLKpYNvAk"
      },
      "source": [
        "# Guide to use BERT model using Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZnYzyjoSH0F"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dri6P8bCt7Di",
        "outputId": "276ebba2-8e93-4ce4-ec7a-10339ba273da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L71c4v7KSMui"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CwpCpxYxJ_CE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VGwdmELJNtTQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xD3otTdScNd"
      },
      "source": [
        "## Example-1: Use BERT Tokenizer and BERT pre-trained models\n",
        "\n",
        "### Load the pre-trained BERT model and its tokenizer.\n",
        "\n",
        "The model and tokenizer should be specifically designed for question-answering tasks:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet does the following:\n",
        "\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\": This line assigns the string \"bert-large-uncased-whole-word-masking-finetuned-squad\" to the variable model_name. This string is a specific identifier for a pre-trained BERT model available in the Hugging Face Transformers library. This particular model is a large, uncased version of BERT that has been fine-tuned on the SQuAD (Stanford Question Answering Dataset) dataset, making it suitable for question-answering tasks.\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name): This line loads the tokenizer associated with the specified model_name. The BertTokenizer is responsible for converting raw text into numerical representations (tokens and their IDs) that the BERT model can understand. The from_pretrained() method downloads and loads the pre-trained tokenizer files from the Hugging Face model hub based on the provided model_name.\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name): This line loads the pre-trained BERT model for question answering. The BertForQuestionAnswering class is a specific BERT model architecture designed for question-answering tasks. Similar to the tokenizer, the from_pretrained() method downloads and loads the pre-trained model weights from the Hugging Face model hub based on the model_name.\n",
        "In essence, these lines prepare the necessary components (tokenizer and model) for performing question answering using the specified BERT model."
      ],
      "metadata": {
        "id": "FT0s4VRolrCH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzehpdEMP4WI",
        "outputId": "df4cf1e0-c664-407c-b9a5-abe0bd6547ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Transformer model, a checkpoint would include:\n",
        "1. The model architecture (or at least the parameters that define it, such as the number of layers, attention heads, etc.)\n",
        "2. The model's parameters (weights and biases) at the time of saving.\n",
        "3. The state of the optimizer (if we want to resume training).\n",
        "4. Other training state information like the current epoch, step, learning rate, etc."
      ],
      "metadata": {
        "id": "M3CQ4-a-nDh-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkPq_6noUuKg"
      },
      "source": [
        "### Tokenization:\n",
        "\n",
        "Tokenization is the process of converting raw text into a format suitable for input to the model. For BERT, this involves breaking text into tokens and mapping them to their corresponding IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W53AHFIHVfK0"
      },
      "outputs": [],
      "source": [
        "# Example context\n",
        "context = \"\"\"\n",
        "AI Planet is a global AI community with headquarters in Belgium and India.\n",
        "It started with the vision to make AI education accessible to everyone and build AI for good to solve key challenges of humanity.\n",
        "As part of our community initiatives, we provide free AI and data science courses by industry experts from large tech companies or\n",
        "startups worldwide. Over 300K+ learners from 150+ countries have benefited since our inception in 2020.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zFo69pk8VgHK"
      },
      "outputs": [],
      "source": [
        "question = \"What is the vision of AI Planet?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RlGg2_vMUqtq"
      },
      "outputs": [],
      "source": [
        "#tokenize\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URsbRlX9V73D"
      },
      "source": [
        "## Model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EluGzRfAVqPr"
      },
      "outputs": [],
      "source": [
        "output = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet does the following:\n",
        "\n",
        "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\": This line assigns the string \"bert-large-uncased-whole-word-masking-finetuned-squad\" to the variable model_name. This string is a specific identifier for a pre-trained BERT model available in the Hugging Face Transformers library. This particular model is a large, uncased version of BERT that has been fine-tuned on the SQuAD (Stanford Question Answering Dataset) dataset, making it suitable for question-answering tasks.\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name): This line loads the tokenizer associated with the specified model_name. The BertTokenizer is responsible for converting raw text into numerical representations (tokens and their IDs) that the BERT model can understand. The from_pretrained() method downloads and loads the pre-trained tokenizer files from the Hugging Face model hub based on the provided model_name.\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name): This line loads the pre-trained BERT model for question answering. The BertForQuestionAnswering class is a specific BERT model architecture designed for question-answering tasks. Similar to the tokenizer, the from_pretrained() method downloads and loads the pre-trained model weights from the Hugging Face model hub based on the model_name.\n",
        "In essence, these lines prepare the necessary components (tokenizer and model) for performing question answering using the specified BERT model.\n",
        "\n",
        "\n",
        "Please explain this code:\n",
        "\n",
        "This code snippet output = model(**inputs) does the following:\n",
        "\n",
        "model: This refers to the pre-trained BERT model for question answering that was loaded in a previous step.\n",
        "inputs: This is a dictionary containing the tokenized and encoded input data, which includes the question and the context. The ** before inputs is the Python \"unpacking\" operator. It unpacks the key-value pairs from the inputs dictionary and passes them as keyword arguments to the model.\n",
        "output: This variable stores the output generated by the BERT model after processing the input. For a question-answering model, this output typically includes logits (scores) for the start and end positions of the answer within the input context."
      ],
      "metadata": {
        "id": "e7ed8Wi_nYOG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMqbjiaV_dT"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls8EwwClV2pS",
        "outputId": "f702c6cb-0869-4f67-db5f-30783f2036ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-6.2164, -4.6178, -7.4844, -7.4729, -7.8819, -8.5008, -8.3274, -9.4713,\n",
              "         -9.5067, -6.2164, -0.8900, -4.5244, -6.1605, -5.0710, -3.4343, -4.2045,\n",
              "         -5.6482, -7.3996, -6.0562, -7.8513, -5.3013, -8.0156, -4.7465, -6.2164,\n",
              "          2.3201, -1.2765, -1.3671,  3.1735,  1.5344,  5.9816,  6.5448,  1.2525,\n",
              "         -0.4587, -0.9794, -3.6875, -2.2243, -3.2219,  2.1678, -1.9828, -4.5240,\n",
              "         -2.9454,  0.8003, -0.1549, -3.1594, -3.3971, -6.1947, -2.4343, -4.1118,\n",
              "         -5.3309, -6.9898, -8.1613, -5.1530, -6.0295, -5.9806, -8.0785, -3.5412,\n",
              "         -4.1596, -4.5303, -5.6748, -8.4360, -6.4948, -7.5843, -5.4967, -8.4173,\n",
              "         -7.1801, -7.2999, -8.6525, -7.7577, -7.4749, -7.8930, -8.6425, -6.6666,\n",
              "         -8.8628, -7.4559, -7.4648, -5.2379, -7.1483, -8.4080, -8.1202, -6.1591,\n",
              "         -8.1705, -7.4948, -8.5753, -7.3179, -7.7387, -6.0562, -7.3644, -6.8786,\n",
              "         -5.9852, -7.3483, -4.0782, -8.1760, -6.2161]],\n",
              "       grad_fn=<CloneBackward0>), end_logits=tensor([[-1.1940, -5.4294, -7.0476, -8.1381, -7.7240, -7.7449, -8.2318, -7.6105,\n",
              "         -7.7720, -1.1940, -4.9784, -4.1272, -6.9474, -7.0882, -5.5077, -3.6932,\n",
              "         -3.0980, -6.5499, -5.6403, -7.0041, -4.9438, -7.2101, -3.6284, -1.1938,\n",
              "         -5.7182, -4.9916, -5.3961, -4.1791, -2.6486, -2.5010, -2.0811, -2.7683,\n",
              "          0.1799,  0.1584, -2.8163,  5.3606, -3.0140, -2.4939, -0.1126, -3.0470,\n",
              "          4.5243, -2.9455, -2.4059, -3.2216,  1.6337, -3.1933,  6.2993,  4.7524,\n",
              "         -8.0098, -7.2664, -7.6783, -7.5033, -6.2816, -3.6608, -4.7226, -7.2637,\n",
              "         -6.3836, -5.0059, -4.5097, -7.7335, -6.5389, -4.0114, -2.5908, -6.8238,\n",
              "         -6.2123, -3.9447, -7.0729, -6.9573, -6.4008, -4.9341, -7.4264, -6.6831,\n",
              "         -5.6289, -2.9392, -3.1973, -7.3668, -7.1777, -7.1921, -7.0572, -4.0202,\n",
              "         -7.8034, -6.7175, -7.1701, -5.3238, -7.2551, -4.6627, -7.5020, -7.1722,\n",
              "         -5.1047, -7.8293, -3.2500, -3.7046, -1.1951]],\n",
              "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kjUY5JlOWIb_"
      },
      "outputs": [],
      "source": [
        "answer_start_index = int(torch.argmax(output.start_logits, axis=-1)[0])\n",
        "answer_end_index = int(torch.argmax(output.end_logits, axis=-1)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iObMPGwpevYt"
      },
      "source": [
        "- **output.start_logits**: likely represents the output scores or logits generated by the model for the starting position of the answer span in the input passage.\n",
        "-  **output.end_logits** represents the output scores or logits generated by the model for the ending position of the answer span in the input passage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUmAbBBiWCL"
      },
      "source": [
        "The returned resoibse is unique IDs for the token. Using Tokenization decode every id into text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gnkODyZNRd0I",
        "outputId": "84881f47-1766-4180-e6ff-f72ccebd6779"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'make ai education accessible to everyone and build ai for good to solve key challenges of humanity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "predict = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTcRUJaFkiem"
      },
      "source": [
        "### Important Note:\n",
        "\n",
        "The Transformers library ensures compatibility between tokenizers and models. When selecting a model and tokenizer pair, make sure they are compatible and intended for the same architecture. For example, you should use a BERT tokenizer with a BERT model, a GPT-2 tokenizer with a GPT-2 model, and so on.\n",
        "\n",
        "The Hugging Face model hub provides a variety of pre-trained models and their associated tokenizers. You can find the specific model name in the documentation. For instance, if you're using BERT for sequence classification, you might use bert-base-uncased for both the tokenizer and the model.\n",
        "\n",
        "Remember to check the specific model's documentation for usage details related to inputs, outputs, and special features, as they can vary based on the model's architecture and intended task. The Transformers library documentation and Hugging Face's GitHub repository are excellent resources to explore more about different models, tokenizers, and their applications."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}